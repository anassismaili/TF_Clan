# -*- coding: utf-8 -*-
"""Project2: MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1775MYdu40ztLgUU8xwsg7aHFPX84UEd1
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal as mvn
import random
import seaborn as sns
import math
from google.colab import drive

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Proyect 2/Anass Ismaili - MNIST_train.csv')
df.head()

df.columns

df.isna().sum()

df = df.drop("Unnamed: 0", axis=1)

df.head()

df = df.drop("index", axis=1)

df.head()

def min_max_scaling(X):
    new_X =[]
    for i in X:
      new_X.append((i - i.min())/(i.max() - i.min()))
    return np.asarray(new_X)

X_train = df.to_numpy()

#X_train = min_max_scaling(X_train)

X_train

y_train = X_train[:,0]

y_train

type(y_train)

sns.countplot(y_train, palette="pastel", edgecolor=".6", color="c")

#def count_du(numbers):
#    count = 0
#    for n in set(numbers):
#      if numbers.count(n) > 0:
#        count += 1
#    return count

#df.groupby('Label').filter(lambda x: x['Values'].count() > 1)

X = df.drop("labels", axis=1)

X

X_train

Xtr = df.drop("labels", axis=1)

X_train = Xtr.to_numpy()

X_train

img = np.reshape(X_train[7], (28,28))

plt.imshow(img)

class KNNClassifier():
  def fit(self, x,y):
    self.x=x
    self.y=y.astype(int)
  def predict(self,x,k,epsilon=1e-3):
    N=len(x)
    y_hat = np.zeros(N)
    for i in range(N):
      #distance between one point to all other point
      dist2=np.sum((self.x-x[i])**2, axis=1)
      idxt=np.argsort(dist2)[:k]
      gamma_k = 1/(np.sqrt(dist2[idxt] + epsilon))
      y_hat[i] = np.bincount(self.y[idxt], weights = gamma_k).argmax()
    return y_hat

Knn = KNNClassifier()

Knn.fit(X_train, y_train)

df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Proyect 2/Anass Ismaili - MNIST_test.csv')
df1.head()

df1 = df1.drop("index", axis=1)

df1=df1.drop("Unnamed: 0", axis=1)

df1.head()

X_test = df1.to_numpy()

X_test

y_test=X_test[:,0]

y_test

df1.head()

Xt = df1.drop("labels", axis=1)

X_test = Xt.to_numpy()

X_test

X_test

X_test.shape

y_hat_train = Knn.predict(X_train, 10)

y_hat_test = Knn.predict(X_test, 10)

def accuracy(y, y_hat):
  return np.mean(y==y_hat)

accuracy(y_train, y_hat_train)

plt.figure(figsize=(10,7))
y_actu = pd.Series(y_train, name='Actual')
y_pred = pd.Series(y_hat_train, name='Predicted')
cm = pd.crosstab(y_train, y_hat_train)
ax = sns.heatmap(cm, annot=True, fmt="d")
plt.ylabel('True label')
plt.xlabel('Predicted label')

accuracy(y_test, y_hat_test)

plt.figure(figsize=(10,7))
y_actu = pd.Series(y_test, name='Actual')
y_pred = pd.Series(y_hat_test, name='Predicted')
cm = pd.crosstab(y_test, y_hat_test)
ax = sns.heatmap(cm, annot=True, fmt="d")
plt.ylabel('True label')
plt.xlabel('Predicted label')

"""### Naive Bayes"""

class GaussNB():
  def fit(self, X, y, epsilon=1e-3):

    self.likelihoods = dict ()
    self.priors = dict()

    self.K = set(y.astype(int))

    for k in self.K:

      X_k = X[y == k, : ]

      self.likelihoods[k] = {'mean': X_k.mean(axis = 0), "cov": X_k.var(axis = 0) + epsilon}
      self.priors[k] = len(X-k)/len(X)


  def predict(self, X):

    N, D = X.shape

    P_hat = np.zeros((N, len(self.K)))

    for k, l in self.likelihoods.items():
      # Bayes Theorem application
      P_hat[:, k] = mvn.logpdf(X, l['mean'], l['cov']) + np.log(self.priors[k])
    
    return P_hat.argmax(axis = 1)

nb = GaussNB()

xtr2 = min_max_scaling(X_train)

ytr2 = xtr2[:,0]

xtr2

ytr2

y_train

X_train

nb.fit(X_train, y_train)

nb.fit(xtr2, ytr2)

X_test

X_test.shape

y_hat_tr2 = nb.predict(xtr2)

y_hat_train2 = nb.predict(X_train)

y_hat_test2 = nb.predict(X_test)

accuracy(y_test, y_hat_test2)

plt.figure(figsize=(10,7))
y_actu = pd.Series(y_test, name='Actual')
y_pred = pd.Series(y_hat_test2, name='Predicted')
cm = pd.crosstab(y_test, y_hat_test2)
ax = sns.heatmap(cm, annot=True, fmt="d")
plt.ylabel('True label')
plt.xlabel('Predicted label')

accuracy(y_train, y_hat_train2)

plt.figure(figsize=(10,7))
y_actu = pd.Series(y_train, name='Actual')
y_pred = pd.Series(y_hat_train2, name='Predicted')
cm = pd.crosstab(y_train, y_hat_train2)
ax = sns.heatmap(cm, annot=True, fmt="d")
plt.ylabel('True label')
plt.xlabel('Predicted label')

"""### Gauss Bayes"""

class GaussBayes():
  def fit(self, X, y, epsilon=1e-3):
    self.likelihoods = dict()
    self.priors=dict()
    self.K=set(y.astype(int))

    for k in self.K:
      X_k = X[y==k, :]
      N_k, D =X_k.shape
      mu_k = X_k.mean(axis=0)
      self.likelihoods[k] = {"mean": X_k.mean(axis=0), "cov": (1/(N_k-1))*np.matmul((X_k-mu_k).T, X_k-mu_k) + epsilon*np.identity(D)} 
      self.priors[k] = len(X_k)/len(X)
  def predict(self, X):
    N,D=X.shape
    P_hat = np.zeros((N, len(self.K)))
    for k, l in self.likelihoods.items():
      P_hat[:,k] = mvn.logpdf(X, l["mean"], l["cov"]) + np.log(self.priors[k])
    return P_hat.argmax(axis=1)

GB = GaussBayes()

GB.fit(X_train, y_train, epsilon=1e-3)

y_hat_train3 = GB.predict(X_train)

y_hat_test3 = GB.predict(X_test)

accuracy(y_train, y_hat_train3)

plt.figure(figsize=(10,7))
y_actu = pd.Series(y_train, name='Actual')
y_pred = pd.Series(y_hat_train3, name='Predicted')
cm = pd.crosstab(y_train, y_hat_train3)
ax = sns.heatmap(cm, annot=True, fmt="d")
plt.ylabel('True label')
plt.xlabel('Predicted label')

accuracy(y_test, y_hat_test3)

plt.figure(figsize=(10,7))
y_actu = pd.Series(y_test, name='Actual')
y_pred = pd.Series(y_hat_test3, name='Predicted')
cm = pd.crosstab(y_test, y_hat_test3)
ax = sns.heatmap(cm, annot=True, fmt="d")
plt.ylabel('True label')
plt.xlabel('Predicted label')