# -*- coding: utf-8 -*-
"""Project3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1egQsl_iJy4i-2-y0cTpZWXGpwjz0bAt3
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal as mvn
import random
import seaborn as sns
import math
from google.colab import drive
from scipy import stats

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Clean_data/raw_house_data.csv')
df.shape

df.columns

df.hist(figsize=(10,5))

df.describe()

df.dtypes

df.isna().sum()

df['HOA'].unique()

df['garage'].unique()

df['bathrooms'].unique()

df['kitchen_features'].unique()

df['garage'].replace('None', np.nan, inplace=True)
df['bathrooms'].replace('None', np.nan, inplace=True)
df['sqrt_ft'].replace('None', np.nan, inplace=True)

df['garage'].unique()

df['HOA'].unique()

df['HOA'].replace('None', np.nan, inplace=True)

df['HOA'].replace(',','', regex=True)

df['HOA'].dtypes

df.isna().sum()

corr = df.corr()
plt.figure(figsize=(10,5))
sns.heatmap(corr, annot = True)

df['HOA']

df['HOA'].replace(np.nan, '-1', inplace=True)

df['HOA']

df['HOA'].unique()

df['HOA'] = df.HOA.replace(',','', regex=True)

df['HOA'].unique()

df['HOA'].astype(float)

df.isna().sum()

df["sqrt_ft"] = df.sqrt_ft.astype(float)
df["bathrooms"] = df.bathrooms.astype(float)
df["garage"] = df.garage.astype(float)

#col_mean = df["bathrooms"].mean()

#df["bathrooms"].fillna(col_mean, inplace = True)

df.isna().sum()

df["bathrooms"].hist(bins = 100)

col_median = df["bathrooms"].median()

df['bathrooms'].fillna(col_median, inplace=True)

df.isna().sum()

df["bathrooms"].hist(bins = 100)

df['bathrooms'].unique()

df[df['bathrooms'] == 35.]

df[df['bathrooms'] == 36.]

df = df.drop(df.index[[2106, 2111, 3277, 4663]], axis= 0)

df.head()

df[df['bathrooms'] == 36.]

df[df['bathrooms'] == 35.]

df.isna().sum()

df['garage'].unique()

df['bathrooms'].unique()

df['bathrooms'].hist(bins = 60, figsize=(20, 10))

df.isna().sum()

col_mean3= df["lot_acres"].mean()
df["lot_acres"].unique()

df['lot_acres'].hist(bins = 60, figsize=(20, 10))

df['lot_acres'].fillna(col_mean3, inplace=True)

df['lot_acres'].hist(bins = 60, figsize=(20, 10))

col_median3 = df["lot_acres"].median()

df['lot_acres'].fillna(col_mean3, inplace=True)

df['lot_acres'].hist(bins = 60, figsize=(20, 10))

df.isna().sum()

df['garage'].unique()

df['garage'].hist(bins = 60, figsize=(20, 10))

col_mean4= df["garage"].mean()
df["garage"].fillna(col_mean4, inplace=True)
df['garage'].hist(bins = 100, figsize=(20, 10))

col_median4= df["garage"].median()
df["garage"].fillna(col_median4, inplace=True)
df['garage'].hist(bins = 100, figsize=(20, 10))

df.isna().sum()

df['sqrt_ft'].unique()

df['sqrt_ft'].dtype

col_median5= df['sqrt_ft'].median()
df['sqrt_ft'].fillna(col_median5, inplace=True)
df['sqrt_ft'].hist(bins = 100, figsize=(20, 10))

df.isna().sum()

df.head()

df = df.drop('kitchen_features', axis=1)

df = df.drop('floor_covering', axis=1)

df.shape

corr = df.corr()
plt.figure(figsize=(12,6))
sns.heatmap(corr, annot = True)

df['lot_acres'].unique()

df['N_F'] = df['sold_price']/df['sqrt_ft']

df.head()

df['N_F'].hist(bins = 60, figsize=(16, 8))

def classifier(columns):
  if columns <= 200:
    return 1
  elif columns > 200 and columns <= 400:
    return 2
  elif columns > 400 and columns <= 600:
    return 3
  else:
    return 4

df['C-F']=df['N_F'].map(lambda x: classifier(x))

corr = df.corr()
plt.figure(figsize=(20,8))
sns.heatmap(corr, annot = True)

df.head(100)

df.columns

ndf = df.loc[:,[ 'sold_price', 'longitude', 'latitude', 'bedrooms', 'bathrooms', 'sqrt_ft', 'C-F']]

X = ndf.to_numpy()
random.shuffle(X)
train_data = X[:int((len(X)+1)*.90)] #Remaining 90% to training set
test_data = X[int((len(X)+1)*.90):] #Splits 10% data to test set

train_data.shape

test_data.shape

X

X.shape

class KNNClassifier():
  def fit(self, x,y):
    self.x=x
    self.y=y.astype(int)
  def predict(self,x,k,epsilon=1e-3):
    N=len(x)
    y_hat = np.zeros(N)
    for i in range(N):
      #distance between one point to all other point
      dist2=np.sum((self.x-x[i])**2, axis=1)
      idxt=np.argsort(dist2)[:k]
      gamma_k = 1/(np.sqrt(dist2[idxt] + epsilon))
      y_hat[i] = np.bincount(self.y[idxt], weights = gamma_k).argmax()
    return y_hat

Gnb = KNNClassifier()

train_data

y_train = train_data[:,-1]

y_train

X_train = train_data[:,:-1]

def min_max_scaling(column):
    return (column - column.min())/(column.max() - column.min())

X_train

X_train

Gnb.fit(X_train, y_train)

y_test = test_data[:,-1]

y_test

X_test = test_data[:,:-1]

X_test

X_test

y_hat1 = Gnb.predict(X_test, 15)

y_hat1.shape

y_hat2 = Gnb.predict(X_train, 10)

def accuracy(y, y_hat):
  return np.mean(y==y_hat)

accuracy(y_test, y_hat1)

accuracy(y_train, y_hat2)

range1 = (df['C-F'] == 1)

range1_df = df.loc[range1, ['MLS', 'sold_price', 'zipcode', 'longitude', 'latitude', 'lot_acres', 'taxes', 'year_built', 'bedrooms', 'bathrooms', 'sqrt_ft', 'garage','fireplaces', 'N_F', 'C-F']]

range1_df.shape

range1_df.head()

range2 = (df['C-F'] == 2)

range2_df = df.loc[range2, ['MLS', 'sold_price', 'zipcode', 'longitude', 'latitude', 'lot_acres', 'taxes', 'year_built', 'bedrooms', 'bathrooms', 'sqrt_ft', 'garage','fireplaces', 'N_F', 'C-F']]

range2_df.shape

range3 = (df['C-F'] == 3)

range3_df = df.loc[range3, ['MLS', 'sold_price', 'zipcode', 'longitude', 'latitude', 'lot_acres', 'taxes', 'year_built', 'bedrooms', 'bathrooms', 'sqrt_ft', 'garage','fireplaces','N_F', 'C-F']]

range3_df.shape

range4 = (df['C-F'] == 4)

range4_df = df.loc[range4, ['MLS', 'sold_price', 'zipcode', 'longitude', 'latitude', 'lot_acres', 'taxes', 'year_built', 'bedrooms', 'bathrooms', 'sqrt_ft', 'garage','fireplaces','N_F', 'C-F']]

range4_df.shape

class OurLinearRegression():
  def fit(self, X, y, eta=1e-3, epochs=1e3, show_curve= False):
    epochs = int(epochs)
    N, D = X.shape
    Y = y
    #Stochastic Gradient Decendent
    #Initialize the weights
    self.W = np.random.randn(D)

    J=np.zeros(epochs)

    #Gradient Descent Step
    for epoch in range(epochs):
      Y_hat = self.predict(X)
      J[epoch] = OLS(Y, Y_hat, N) #Calculate Error/loss
      #Weights update rule
      self.W -= eta*(1/N)*(X.T@(Y_hat-Y))
    if show_curve:
      plt.figure()
      plt.plot(J)
      plt.xlabel("epochs")
      plt.ylabel("$\mathcal{J}$")
      plt.title("Training Curve")
      plt.show()
  def predict(self, X):
    return X@self.W

def OLS(Y, Y_hat, N):
  return (1/(2^N)*np.sum((Y-Y_hat)**2))
def R2(Y, Y_hat):
  return (1-(np.sum((Y-Y_hat)**2)/np.sum((Y-np.mean(Y))**2)))

"""### Range1"""

range1_df.columns

X_range1 = range1_df[['sqrt_ft', 'bedrooms' , 'lot_acres', 'N_F','sold_price']].copy()

X_range1.shape

def min_max_scaling(column):
    return (column - column.min())/(column.max() - column.min())

XR1 = X_range1.to_numpy()
#XR1 = min_max_scaling(XR1)
random.shuffle(XR1)
trainR1_data = XR1[:int((len(XR1)+1)*.90)] #Remaining 90% to training set
testR1_data = XR1[int((len(XR1)+1)*.90):] #Splits 10% data to test set

trainR1_data.shape

testR1_data.shape

corr = X_range1.corr()
plt.figure(figsize=(20,8))
sns.heatmap(corr, annot = True)

y_range1Tr=trainR1_data[:,0]

y_range1Tr

y_range1Tr.shape

X_range1Tr=trainR1_data[:,1:]

X_range1Tr.shape

y_range1Te=testR1_data[:,0]

y_range1Te.shape

X_range1Te=testR1_data[:,1:]

X_range1Te.shape

Reg = OurLinearRegression()

Reg.fit(X_range1Tr, y_range1Tr, epochs = 1e3, eta=1e-14, show_curve=True)

y_hatR1Te=Reg.predict(X_range1Te)

R2(y_range1Te,y_hatR1Te)

y_hatR1Tr=Reg.predict(X_range1Tr)

R2(y_range1Tr,y_hatR1Tr)

"""### Range 2"""

X_range2 = range2_df[['sqrt_ft', 'bedrooms' , 'N_F', 'sold_price']].copy()

X_range2.shape

XR2 = X_range2.to_numpy()
#XR2 = min_max_scaling(XR2)
random.shuffle(XR1)
trainR2_data = XR2[:int((len(XR2)+1)*.90)] #Remaining 90% to training set
testR2_data = XR2[int((len(XR2)+1)*.90):] #Splits 10% data to test set

y_range2Tr=trainR2_data[:,0]

X_range2Tr=trainR2_data[:,1:]

y_range2Te=testR2_data[:,0]

X_range2Te=testR2_data[:,1:]

Reg.fit(X_range2Te, y_range2Te, epochs = 1e4, eta=1e-14, show_curve=True)

y_hatR2Te=Reg.predict(X_range2Te)

y_hatR2Tr=Reg.predict(X_range2Tr)

R2(y_range2Te,y_hatR2Te)

R2(y_range2Tr,y_hatR2Tr)

"""### Range 3"""

X_range3 = range3_df[['sqrt_ft', 'bedrooms', 'lot_acres', 'N_F', 'sold_price']].copy()

XR3 = X_range3.to_numpy()
#XR2 = min_max_scaling(XR2)
random.shuffle(XR1)
trainR3_data = XR3[:int((len(XR3)+1)*.90)] #Remaining 90% to training set
testR3_data = XR3[int((len(XR3)+1)*.90):] #Splits 10% data to test set

X_range4 = range4_df[['sqrt_ft', 'bedrooms', 'lot_acres', 'N_F', 'sold_price']].copy()

X_range3 = X_range3.to_numpy()

X_range4 = X_range4.to_numpy()

y_range3Tr=trainR3_data[:,0]

X_range3Tr=trainR3_data[:,1:]

y_range3Te=testR3_data[:,0]

X_range3Te=testR3_data[:,1:]

Reg.fit(X_range3Te, y_range3Te, epochs = 1e3, eta=1e-14, show_curve=True)

y_hatR3Te=Reg.predict(X_range3Te)

y_hatR3Tr=Reg.predict(X_range3Tr)

R2(y_range3Te,y_hatR3Te)

R2(y_range3Tr,y_hatR3Tr)

X_range4.shape